{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f070c8c1-6341-4be4-abdd-01359889d38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Using cached librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Using cached audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Downloading numba-0.60.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: numpy>=1.22.3 in ./anaconda3/envs/tf_macos/lib/python3.9/site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./anaconda3/envs/tf_macos/lib/python3.9/site-packages (from librosa) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in ./anaconda3/envs/tf_macos/lib/python3.9/site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in ./anaconda3/envs/tf_macos/lib/python3.9/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./anaconda3/envs/tf_macos/lib/python3.9/site-packages (from librosa) (5.1.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Using cached soundfile-0.13.1-py2.py3-none-macosx_11_0_arm64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Using cached pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-0.5.0.post1-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in ./anaconda3/envs/tf_macos/lib/python3.9/site-packages (from librosa) (4.12.2)\n",
      "Collecting lazy_loader>=0.1 (from librosa)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Downloading msgpack-1.1.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: packaging in ./anaconda3/envs/tf_macos/lib/python3.9/site-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Downloading llvmlite-0.43.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./anaconda3/envs/tf_macos/lib/python3.9/site-packages (from pooch>=1.1->librosa) (4.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./anaconda3/envs/tf_macos/lib/python3.9/site-packages (from pooch>=1.1->librosa) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./anaconda3/envs/tf_macos/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/envs/tf_macos/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/envs/tf_macos/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/envs/tf_macos/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.4.26)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./anaconda3/envs/tf_macos/lib/python3.9/site-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in ./anaconda3/envs/tf_macos/lib/python3.9/site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in ./anaconda3/envs/tf_macos/lib/python3.9/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Using cached librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Using cached audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading msgpack-1.1.1-cp39-cp39-macosx_11_0_arm64.whl (78 kB)\n",
      "Downloading numba-0.60.0-cp39-cp39-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading llvmlite-0.43.0-cp39-cp39-macosx_11_0_arm64.whl (28.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.8/28.8 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Using cached soundfile-0.13.1-py2.py3-none-macosx_11_0_arm64.whl (1.1 MB)\n",
      "Downloading soxr-0.5.0.post1-cp39-cp39-macosx_11_0_arm64.whl (160 kB)\n",
      "Installing collected packages: soxr, msgpack, llvmlite, lazy_loader, audioread, soundfile, pooch, numba, librosa\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9/9\u001b[0m [librosa]m7/9\u001b[0m [numba]ile]\n",
      "\u001b[1A\u001b[2KSuccessfully installed audioread-3.0.1 lazy_loader-0.4 librosa-0.11.0 llvmlite-0.43.0 msgpack-1.1.1 numba-0.60.0 pooch-1.8.2 soundfile-0.13.1 soxr-0.5.0.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "371e73c5-0e3e-443d-ae9a-18d6cd7736f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Audio processing and feature extraction\n",
    "import librosa\n",
    "import soundfile\n",
    "\n",
    "# File and data handling\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "# Numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c04b5d9-a078-4099-becf-ffe8b60e045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    X, sample_rate = librosa.load(file_name, res_type='scipy')\n",
    "\n",
    "    if chroma:\n",
    "        stft = np.abs(librosa.stft(X))\n",
    "    result = np.array([])\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result = np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, mel))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5abdc290-4b40-400e-bccb-6f945a10a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "observed_emotions = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fe7ff1d-24a9-48f2-9360-f607b0588ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech files found: 1440\n",
      "Song files found: 1012\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "speech_files = glob.glob('/Users/dushyantyadav/Downloads/Audio_Speech_Actors_01-24/Actor*/**/*.wav', recursive=True)\n",
    "song_files = glob.glob('/Users/dushyantyadav/Downloads/Audio_Song_Actors_01-24/Actor*/**/*.wav', recursive=True)\n",
    "print(\"Speech files found:\", len(speech_files))\n",
    "print(\"Song files found:\", len(song_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7ee9f4a-b6e7-4f5a-a83c-9c1c4f229e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(test_size=0.2):\n",
    "    x, y = [], []\n",
    "    # Pattern for speech files\n",
    "    speech_files = glob.glob('/Users/dushyantyadav/Downloads/Audio_Speech_Actors_01-24/Actor*/**/*.wav', recursive=True)\n",
    "    # Pattern for song files\n",
    "    song_files = glob.glob('/Users/dushyantyadav/Downloads/Audio_Song_Actors_01-24/Actor*/**/*.wav', recursive=True)\n",
    "    # Combine both file lists\n",
    "    all_files = speech_files + song_files\n",
    "\n",
    "    for file in all_files:\n",
    "        file_name = os.path.basename(file)\n",
    "        emotion = emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        feature = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6ce1c448-b5b5-49dd-9f49-286b297f7e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9d0ca0d1-d603-422b-ace5-e578a17c41a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1961, 491)\n"
     ]
    }
   ],
   "source": [
    "print((x_train.shape[0], x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7d92d4ce-2034-4ec1-b95c-778250d43c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted: 180\n"
     ]
    }
   ],
   "source": [
    "print(f'Features extracted: {x_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e719719-9333-4ba8-a3c8-a51ba7d805ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b4e9ae9-d61c-4d40-acb9-dbc1216379f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_sequence(file_path, n_mfcc=40, max_len=200):\n",
    "    y, sr = librosa.load(file_path, res_type='scipy')\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    # Pad or truncate to fixed length for batching\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0,0),(0,pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    return mfcc.T  # Shape: (max_len, n_mfcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2087681e-a924-4ec8-9fa0-df051b38b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {\n",
    "    '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
    "    '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
    "}\n",
    "observed_emotions = list(emotions.values())\n",
    "\n",
    "def load_data_dl(test_size=0.2, max_len=200):\n",
    "    x, y = [], []\n",
    "    # Update these paths for your dataset\n",
    "    speech_files = glob.glob('/Users/dushyantyadav/Downloads/Audio_Speech_Actors_01-24/Actor*/**/*.wav', recursive=True)\n",
    "    song_files = glob.glob('/Users/dushyantyadav/Downloads/Audio_Song_Actors_01-24/Actor*/**/*.wav', recursive=True)\n",
    "    all_files = speech_files + song_files\n",
    "    for file in all_files:\n",
    "        file_name = os.path.basename(file)\n",
    "        emotion_code = file_name.split(\"-\")[2]\n",
    "        emotion = emotions.get(emotion_code)\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        try:\n",
    "            mfcc_seq = extract_mfcc_sequence(file, max_len=max_len)\n",
    "            x.append(mfcc_seq)\n",
    "            y.append(emotion)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)\n",
    "    y_cat = to_categorical(y_enc)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y_cat, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    return x_train, x_test, y_train, y_test, le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0eb543e-e10e-48a2-9e64-9faad53f522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_gru(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, kernel_size=5, activation='relu', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(128, kernel_size=5, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(GRU(128, return_sequences=False))  # <-- GRU replaces LSTM\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6e202-b4b0-47a4-a452-e90c66562ac0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dushyantyadav/anaconda3/envs/tf_macos/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-06-21 20:38:23.074057: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-06-21 20:38:23.075069: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-06-21 20:38:23.075111: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-06-21 20:38:23.075680: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-06-21 20:38:23.076088: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 20:38:24.545889: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 66ms/step - accuracy: 0.1491 - loss: 2.2587 - val_accuracy: 0.1426 - val_loss: 2.3989\n",
      "Epoch 2/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.2286 - loss: 2.1821 - val_accuracy: 0.1752 - val_loss: 2.4319\n",
      "Epoch 3/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.2338 - loss: 1.9692 - val_accuracy: 0.2505 - val_loss: 1.9772\n",
      "Epoch 4/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.2499 - loss: 1.9902 - val_accuracy: 0.2464 - val_loss: 2.0008\n",
      "Epoch 5/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.2736 - loss: 1.8897 - val_accuracy: 0.2729 - val_loss: 2.0035\n",
      "Epoch 6/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.2992 - loss: 1.8852 - val_accuracy: 0.2974 - val_loss: 1.9073\n",
      "Epoch 7/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.3115 - loss: 1.7959 - val_accuracy: 0.3544 - val_loss: 1.6036\n",
      "Epoch 8/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.3269 - loss: 1.7624 - val_accuracy: 0.3320 - val_loss: 1.7793\n",
      "Epoch 9/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.3688 - loss: 1.6895 - val_accuracy: 0.4114 - val_loss: 1.5334\n",
      "Epoch 10/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.3812 - loss: 1.6340 - val_accuracy: 0.4318 - val_loss: 1.4081\n",
      "Epoch 11/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.3715 - loss: 1.5986 - val_accuracy: 0.4582 - val_loss: 1.4810\n",
      "Epoch 12/100\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.4583 - loss: 1.4578 - val_accuracy: 0.4318 - val_loss: 1.4710\n",
      "Epoch 13/100\n",
      "\u001b[1m35/62\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.4632 - loss: 1.4086"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_train, x_test, y_train, y_test, le = load_data_dl(test_size=0.2, max_len=200)\n",
    "input_shape = x_train.shape[1:]  # (max_len, n_mfcc)\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "# Build model\n",
    "model = build_cnn_gru(input_shape, num_classes)\n",
    "\n",
    "# Early stopping for better generalization\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", acc)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a68b63b-fe68-4390-a75a-5ff6a3f4791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e02146b-b73b-47fc-8cef-158cdecc8160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion mapping (adapt for your dataset)\n",
    "emotions = {\n",
    "    '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
    "    '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
    "}\n",
    "observed_emotions = list(emotions.values())\n",
    "DATA_PATHS = [\n",
    "    '/Users/dushyantyadav/Downloads/Audio_Speech_Actors_01-24/Actor*/**/*.wav',\n",
    "    '/Users/dushyantyadav/Downloads/Audio_Song_Actors_01-24/Actor*/**/*.wav'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d64a366-f515-4a8c-87e0-170ff080b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(data, noise_factor=0.005):\n",
    "    noise = np.random.randn(len(data))\n",
    "    return data + noise_factor * noise\n",
    "\n",
    "def shift(data, shift_max=0.2, shift_direction='both'):\n",
    "    shift_amt = np.random.randint(int(len(data) * shift_max))\n",
    "    if shift_direction == 'right':\n",
    "        shift_amt = -shift_amt\n",
    "    elif shift_direction == 'both':\n",
    "        if np.random.randint(0, 2) == 1:\n",
    "            shift_amt = -shift_amt\n",
    "    return np.roll(data, shift_amt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6198cdc7-1d0e-4650-bf76-c207a4a19c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_sequence(file_path, n_mfcc=40, max_len=200):\n",
    "    y, sr = librosa.load(file_path, res_type='scipy')\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    # Pad or truncate for batching\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0,0),(0,pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    return mfcc.T  # (max_len, n_mfcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41330cee-b980-42a2-a18f-086f7f2d9c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_dl(test_size=0.2, max_len=200, augment=True):\n",
    "    x, y = [], []\n",
    "    files = []\n",
    "    for path in DATA_PATHS:\n",
    "        files.extend(glob.glob(path, recursive=True))\n",
    "    for file in files:\n",
    "        file_name = os.path.basename(file)\n",
    "        emotion_code = file_name.split(\"-\")[2]\n",
    "        emotion = emotions.get(emotion_code)\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        try:\n",
    "            # Original\n",
    "            mfcc_seq = extract_mfcc_sequence(file, max_len=max_len)\n",
    "            x.append(mfcc_seq)\n",
    "            y.append(emotion)\n",
    "            if augment:\n",
    "                # Augmented: noise\n",
    "                y_audio, sr = librosa.load(file, res_type='scipy')\n",
    "                mfcc_noise = librosa.feature.mfcc(y=add_noise(y_audio), sr=sr, n_mfcc=40)\n",
    "                if mfcc_noise.shape[1] < max_len:\n",
    "                    pad_width = max_len - mfcc_noise.shape[1]\n",
    "                    mfcc_noise = np.pad(mfcc_noise, pad_width=((0,0),(0,pad_width)), mode='constant')\n",
    "                else:\n",
    "                    mfcc_noise = mfcc_noise[:, :max_len]\n",
    "                x.append(mfcc_noise.T)\n",
    "                y.append(emotion)\n",
    "                # Augmented: shift\n",
    "                y_shift = shift(y_audio)\n",
    "                mfcc_shift = librosa.feature.mfcc(y=y_shift, sr=sr, n_mfcc=40)\n",
    "                if mfcc_shift.shape[1] < max_len:\n",
    "                    pad_width = max_len - mfcc_shift.shape[1]\n",
    "                    mfcc_shift = np.pad(mfcc_shift, pad_width=((0,0),(0,pad_width)), mode='constant')\n",
    "                else:\n",
    "                    mfcc_shift = mfcc_shift[:, :max_len]\n",
    "                x.append(mfcc_shift.T)\n",
    "                y.append(emotion)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)\n",
    "    y_cat = to_categorical(y_enc)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y_cat, test_size=test_size, random_state=42, stratify=y)\n",
    "    return x_train, x_test, y_train, y_test, le, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b977fa2-6dfa-4652-82a6-7912d40e4e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_lstm(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, kernel_size=5, activation='relu', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(128, kernel_size=5, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(128, return_sequences=False))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59da7cca-090f-40d5-99b2-d74420672c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.815410199556541, 1: 0.8145071982281284, 2: 1.5954446854663775, 3: 0.815410199556541, 4: 0.815410199556541, 5: 1.630820399113082, 6: 0.815410199556541, 7: 1.5954446854663775}\n"
     ]
    }
   ],
   "source": [
    "# After loading data\n",
    "x_train, x_test, y_train, y_test, le, y_all = load_data_dl(test_size=0.2, max_len=200, augment=True)\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_labels), y=y_train_labels)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(\"Class weights:\", class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a586e8e-1ac9-49ee-abeb-304483c115f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 21:49:57.622698: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-06-21 21:49:57.622960: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-06-21 21:49:57.622987: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-06-21 21:49:57.623362: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-06-21 21:49:57.623824: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 21:49:59.570151: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 8s 31ms/step - loss: 2.0172 - accuracy: 0.2039 - val_loss: 1.8619 - val_accuracy: 0.2772\n",
      "Epoch 2/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 1.7506 - accuracy: 0.3110 - val_loss: 1.6217 - val_accuracy: 0.3913\n",
      "Epoch 3/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 1.4754 - accuracy: 0.4369 - val_loss: 1.4525 - val_accuracy: 0.4436\n",
      "Epoch 4/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 1.3165 - accuracy: 0.4944 - val_loss: 1.2939 - val_accuracy: 0.5082\n",
      "Epoch 5/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 1.2301 - accuracy: 0.5318 - val_loss: 1.2052 - val_accuracy: 0.5306\n",
      "Epoch 6/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 1.1058 - accuracy: 0.5766 - val_loss: 1.0416 - val_accuracy: 0.6080\n",
      "Epoch 7/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.9971 - accuracy: 0.6101 - val_loss: 1.0168 - val_accuracy: 0.6230\n",
      "Epoch 8/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.9378 - accuracy: 0.6329 - val_loss: 0.9854 - val_accuracy: 0.6155\n",
      "Epoch 9/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.8837 - accuracy: 0.6555 - val_loss: 0.8855 - val_accuracy: 0.6726\n",
      "Epoch 10/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.8551 - accuracy: 0.6611 - val_loss: 1.0103 - val_accuracy: 0.6372\n",
      "Epoch 11/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.8230 - accuracy: 0.6793 - val_loss: 0.8333 - val_accuracy: 0.6929\n",
      "Epoch 12/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.7574 - accuracy: 0.7029 - val_loss: 0.9016 - val_accuracy: 0.6658\n",
      "Epoch 13/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.7348 - accuracy: 0.7175 - val_loss: 0.8794 - val_accuracy: 0.6841\n",
      "Epoch 14/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.7163 - accuracy: 0.7248 - val_loss: 0.8686 - val_accuracy: 0.6929\n",
      "Epoch 15/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.6837 - accuracy: 0.7354 - val_loss: 0.7429 - val_accuracy: 0.7330\n",
      "Epoch 16/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.6576 - accuracy: 0.7459 - val_loss: 0.7098 - val_accuracy: 0.7310\n",
      "Epoch 17/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.6461 - accuracy: 0.7498 - val_loss: 0.7318 - val_accuracy: 0.7289\n",
      "Epoch 18/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.6346 - accuracy: 0.7536 - val_loss: 0.6885 - val_accuracy: 0.7378\n",
      "Epoch 19/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.5647 - accuracy: 0.7838 - val_loss: 0.7534 - val_accuracy: 0.7303\n",
      "Epoch 20/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.5612 - accuracy: 0.7845 - val_loss: 0.6030 - val_accuracy: 0.7779\n",
      "Epoch 21/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.5351 - accuracy: 0.7954 - val_loss: 0.6487 - val_accuracy: 0.7704\n",
      "Epoch 22/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.5351 - accuracy: 0.7984 - val_loss: 0.5648 - val_accuracy: 0.8071\n",
      "Epoch 23/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.5048 - accuracy: 0.8081 - val_loss: 0.5656 - val_accuracy: 0.7921\n",
      "Epoch 24/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.4982 - accuracy: 0.8166 - val_loss: 0.6258 - val_accuracy: 0.7711\n",
      "Epoch 25/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.4825 - accuracy: 0.8188 - val_loss: 0.5579 - val_accuracy: 0.8084\n",
      "Epoch 26/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.4684 - accuracy: 0.8294 - val_loss: 0.6193 - val_accuracy: 0.7779\n",
      "Epoch 27/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.4618 - accuracy: 0.8319 - val_loss: 0.6452 - val_accuracy: 0.7819\n",
      "Epoch 28/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.4070 - accuracy: 0.8482 - val_loss: 0.6116 - val_accuracy: 0.7833\n",
      "Epoch 29/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.4018 - accuracy: 0.8477 - val_loss: 0.5912 - val_accuracy: 0.7969\n",
      "Epoch 30/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.4045 - accuracy: 0.8448 - val_loss: 0.5700 - val_accuracy: 0.8091\n",
      "Epoch 31/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.3800 - accuracy: 0.8637 - val_loss: 0.4663 - val_accuracy: 0.8363\n",
      "Epoch 32/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.3670 - accuracy: 0.8615 - val_loss: 0.5131 - val_accuracy: 0.8159\n",
      "Epoch 33/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.3559 - accuracy: 0.8651 - val_loss: 0.4558 - val_accuracy: 0.8451\n",
      "Epoch 34/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.3639 - accuracy: 0.8693 - val_loss: 0.5204 - val_accuracy: 0.8234\n",
      "Epoch 35/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.3543 - accuracy: 0.8676 - val_loss: 0.6222 - val_accuracy: 0.8166\n",
      "Epoch 36/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.3186 - accuracy: 0.8843 - val_loss: 0.4591 - val_accuracy: 0.8451\n",
      "Epoch 37/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.3203 - accuracy: 0.8834 - val_loss: 0.4807 - val_accuracy: 0.8438\n",
      "Epoch 38/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.3138 - accuracy: 0.8809 - val_loss: 0.4287 - val_accuracy: 0.8478\n",
      "Epoch 39/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.3159 - accuracy: 0.8815 - val_loss: 0.5414 - val_accuracy: 0.8288\n",
      "Epoch 40/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.2920 - accuracy: 0.8926 - val_loss: 0.5384 - val_accuracy: 0.8302\n",
      "Epoch 41/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.2938 - accuracy: 0.8872 - val_loss: 0.4504 - val_accuracy: 0.8587\n",
      "Epoch 42/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.3149 - accuracy: 0.8865 - val_loss: 0.4070 - val_accuracy: 0.8621\n",
      "Epoch 43/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.2695 - accuracy: 0.9038 - val_loss: 0.4167 - val_accuracy: 0.8702\n",
      "Epoch 44/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.2408 - accuracy: 0.9138 - val_loss: 0.4571 - val_accuracy: 0.8553\n",
      "Epoch 45/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.2823 - accuracy: 0.9019 - val_loss: 0.4360 - val_accuracy: 0.8580\n",
      "Epoch 46/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.2567 - accuracy: 0.9060 - val_loss: 0.5442 - val_accuracy: 0.8356\n",
      "Epoch 47/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.2229 - accuracy: 0.9198 - val_loss: 0.4830 - val_accuracy: 0.8553\n",
      "Epoch 48/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.2443 - accuracy: 0.9121 - val_loss: 0.3745 - val_accuracy: 0.8770\n",
      "Epoch 49/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.2368 - accuracy: 0.9130 - val_loss: 0.3859 - val_accuracy: 0.8723\n",
      "Epoch 50/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.2293 - accuracy: 0.9208 - val_loss: 0.5106 - val_accuracy: 0.8478\n",
      "Epoch 51/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.2315 - accuracy: 0.9181 - val_loss: 0.4354 - val_accuracy: 0.8675\n",
      "Epoch 52/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.2259 - accuracy: 0.9198 - val_loss: 0.3363 - val_accuracy: 0.8981\n",
      "Epoch 53/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.2032 - accuracy: 0.9308 - val_loss: 0.3540 - val_accuracy: 0.8825\n",
      "Epoch 54/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.2002 - accuracy: 0.9244 - val_loss: 0.3664 - val_accuracy: 0.8913\n",
      "Epoch 55/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.2054 - accuracy: 0.9256 - val_loss: 0.4094 - val_accuracy: 0.8777\n",
      "Epoch 56/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.1968 - accuracy: 0.9290 - val_loss: 0.4801 - val_accuracy: 0.8492\n",
      "Epoch 57/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.2129 - accuracy: 0.9203 - val_loss: 0.3881 - val_accuracy: 0.8716\n",
      "Epoch 58/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.2004 - accuracy: 0.9274 - val_loss: 0.3725 - val_accuracy: 0.8804\n",
      "Epoch 59/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.1662 - accuracy: 0.9410 - val_loss: 0.4075 - val_accuracy: 0.8777\n",
      "Epoch 60/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.1917 - accuracy: 0.9335 - val_loss: 0.3324 - val_accuracy: 0.8906\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.3324 - accuracy: 0.8906\n",
      "Test accuracy: 0.890625\n",
      "46/46 [==============================] - 1s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.98      0.90      0.94       226\n",
      "        calm       0.89      0.94      0.92       225\n",
      "     disgust       0.94      0.85      0.89       115\n",
      "     fearful       0.90      0.84      0.86       226\n",
      "       happy       0.92      0.88      0.90       226\n",
      "     neutral       0.78      0.92      0.85       113\n",
      "         sad       0.83      0.85      0.84       226\n",
      "   surprised       0.87      0.97      0.92       115\n",
      "\n",
      "    accuracy                           0.89      1472\n",
      "   macro avg       0.89      0.90      0.89      1472\n",
      "weighted avg       0.89      0.89      0.89      1472\n",
      "\n",
      "Confusion Matrix:\n",
      " [[203   1   3   7   6   1   5   0]\n",
      " [  0 212   0   0   3   7   3   0]\n",
      " [  3   5  98   1   2   1   3   2]\n",
      " [  1   1   1 189   3   1  24   6]\n",
      " [  0   2   1   6 200   9   2   6]\n",
      " [  0   5   0   0   0 104   2   2]\n",
      " [  0  12   0   7   4   9 193   1]\n",
      " [  0   0   1   1   0   1   0 112]]\n"
     ]
    }
   ],
   "source": [
    "input_shape = x_train.shape[1:]  # (max_len, n_mfcc)\n",
    "num_classes = y_train.shape[1]\n",
    "model = build_cnn_lstm(input_shape, num_classes)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=60,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weight_dict\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", acc)\n",
    "\n",
    "y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e10067f0-f33b-4b89-9439-d8dae71acda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Predicted emotion: surprised\n"
     ]
    }
   ],
   "source": [
    "def predict_emotion(file_path, model, le, max_len=200):\n",
    "    mfcc_seq = extract_mfcc_sequence(file_path, max_len=max_len)\n",
    "    mfcc_seq = np.expand_dims(mfcc_seq, axis=0)\n",
    "    pred = model.predict(mfcc_seq)\n",
    "    predicted_class = np.argmax(pred)\n",
    "    return le.classes_[predicted_class]\n",
    "\n",
    "# Example usage\n",
    "test_file = '/Users/dushyantyadav/Downloads/Crema/1001_DFA_ANG_XX.wav'\n",
    "print(\"Predicted emotion:\", predict_emotion(test_file, model, le))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1316614-f3d1-464c-9c23-cdc421bf84f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n",
      "Predicted emotion: fearful\n"
     ]
    }
   ],
   "source": [
    "def predict_emotion(file_path, model, le, max_len=200):\n",
    "    mfcc_seq = extract_mfcc_sequence(file_path, max_len=max_len)\n",
    "    mfcc_seq = np.expand_dims(mfcc_seq, axis=0)\n",
    "    pred = model.predict(mfcc_seq)\n",
    "    predicted_class = np.argmax(pred)\n",
    "    return le.classes_[predicted_class]\n",
    "\n",
    "# Example usage\n",
    "test_file = '/Users/dushyantyadav/Downloads/Crema/1076_IEO_SAD_MD.wav'\n",
    "print(\"Predicted emotion:\", predict_emotion(test_file, model, le))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9db3280e-2171-4bbe-845a-77b74f03830b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 62ms/step\n",
      "Predicted emotion: fearful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_emotion(file_path, model, le, max_len=200):\n",
    "    mfcc_seq = extract_mfcc_sequence(file_path, max_len=max_len)\n",
    "    mfcc_seq = np.expand_dims(mfcc_seq, axis=0)\n",
    "    pred = model.predict(mfcc_seq)\n",
    "    predicted_class = np.argmax(pred)\n",
    "    return le.classes_[predicted_class]\n",
    "\n",
    "# Example usage\n",
    "test_file = '/Users/dushyantyadav/Downloads/Crema/1082_IEO_FEA_MD.wav'\n",
    "print(\"Predicted emotion:\", predict_emotion(test_file, model, le))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31160d11-cbf3-4914-ad0d-40e6a11ce045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
