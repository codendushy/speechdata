{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61744919-59fb-4c0e-a17d-b86d3addf674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cb32bae-5f7d-47af-8192-3f6a6d35ed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "observed_emotions = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b61dc050-5de2-447a-a2ca-5897c46aec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, mfcc=True, chroma=True, mel=True):\n",
    "    X, sample_rate = librosa.load(file_name, res_type='scipy')\n",
    "    result = np.array([])\n",
    "    if chroma:\n",
    "        stft = np.abs(librosa.stft(X))\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result = np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        chroma_feat = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, chroma_feat))\n",
    "    if mel:\n",
    "        mel_feat = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, mel_feat))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28fe440c-42c3-4e38-bdda-bc6d68ad167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(data, noise_factor=0.005):\n",
    "    noise = np.random.randn(len(data))\n",
    "    return data + noise_factor * noise\n",
    "\n",
    "def shift(data, shift_max=0.2, shift_direction='both'):\n",
    "    shift = np.random.randint(int(len(data) * shift_max))\n",
    "    if shift_direction == 'right':\n",
    "        shift = -shift\n",
    "    elif shift_direction == 'both':\n",
    "        direction = np.random.randint(0, 2)\n",
    "        if direction == 1:\n",
    "            shift = -shift\n",
    "    return np.roll(data, shift)\n",
    "\n",
    "def stretch(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(data, rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e510398b-2636-47a0-9079-20e53e60fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(test_size=0.2, augment=False):\n",
    "    x, y = [], []\n",
    "    speech_files = glob.glob('/Users/dushyantyadav/Downloads/Audio_Speech_Actors_01-24/Actor*/**/*.wav', recursive=True)\n",
    "    song_files = glob.glob('/Users/dushyantyadav/Downloads/Audio_Song_Actors_01-24/Actor*/**/*.wav', recursive=True)\n",
    "    all_files = speech_files + song_files\n",
    "    for file in all_files:\n",
    "        file_name = os.path.basename(file)\n",
    "        emotion = emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        # Original\n",
    "        feature = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "        # Augmentation (optional)\n",
    "        if augment:\n",
    "            y_audio, sr = librosa.load(file, res_type='scipy')\n",
    "            # Add noise\n",
    "            feature_noise = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "            x.append(feature_noise)\n",
    "            y.append(emotion)\n",
    "            # Shifted\n",
    "            y_shift = shift(y_audio)\n",
    "            temp_file = \"temp_shift.wav\"\n",
    "            librosa.output.write_wav(temp_file, y_shift, sr)\n",
    "            feature_shift = extract_feature(temp_file, mfcc=True, chroma=True, mel=True)\n",
    "            x.append(feature_shift)\n",
    "            y.append(emotion)\n",
    "            os.remove(temp_file)\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be52f06e-c0f0-45dd-ab4e-f3b49ba090e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = load_data(test_size=0.2, augment=False)  # Set augment=True for augmentation\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f76ed573-65c6-4895-a282-4f194974495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "951de79c-2987-4c2a-b9a9-fba07eb9b2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in training data: Counter({'happy': 312, 'angry': 307, 'sad': 301, 'calm': 298, 'fearful': 296, 'surprised': 154, 'disgust': 149, 'neutral': 144})\n"
     ]
    }
   ],
   "source": [
    "print('Class distribution in training data:', Counter(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "923c3f75-95ee-47bd-baaa-ee16c000d773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Best parameters found: {'alpha': 0.01, 'hidden_layer_sizes': (300,), 'learning_rate': 'constant', 'max_iter': 500}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,), (200,), (300,)],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'max_iter': [500, 1000]\n",
    "}\n",
    "mlp = MLPClassifier()\n",
    "clf = GridSearchCV(mlp, param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "clf.fit(x_train_scaled, y_train_enc)\n",
    "print(\"Best parameters found:\", clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "baa124b2-6238-454a-b216-9d4f5cd10445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7433808553971487\n",
      "F1 Score (Macro): 0.7405718936638566\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.71      0.83      0.77        69\n",
      "        calm       0.82      0.82      0.82        78\n",
      "     disgust       0.74      0.65      0.69        43\n",
      "     fearful       0.71      0.69      0.70        80\n",
      "       happy       0.75      0.75      0.75        64\n",
      "     neutral       0.82      0.73      0.77        44\n",
      "         sad       0.69      0.72      0.71        75\n",
      "   surprised       0.73      0.71      0.72        38\n",
      "\n",
      "    accuracy                           0.74       491\n",
      "   macro avg       0.75      0.74      0.74       491\n",
      "weighted avg       0.74      0.74      0.74       491\n",
      "\n",
      "Confusion Matrix:\n",
      " [[57  0  3  3  3  0  1  2]\n",
      " [ 0 64  0  1  2  4  7  0]\n",
      " [ 6  1 28  2  1  1  2  2]\n",
      " [ 9  0  0 55  6  0 10  0]\n",
      " [ 4  4  1  3 48  0  1  3]\n",
      " [ 0  4  1  1  1 32  3  2]\n",
      " [ 2  4  0  9  3  2 54  1]\n",
      " [ 2  1  5  3  0  0  0 27]]\n"
     ]
    }
   ],
   "source": [
    "best_model = clf.best_estimator_\n",
    "best_model.fit(x_train_scaled, y_train_enc)\n",
    "y_pred = best_model.predict(x_test_scaled)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_enc, y_pred))\n",
    "print(\"F1 Score (Macro):\", f1_score(y_test_enc, y_pred, average='macro'))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_enc, y_pred, target_names=le.classes_))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_enc, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b73a206f-64bf-43a9-bfe3-64f96a5063e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a31879ce-ff7e-48f2-86c5-674f2d0c5696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_sequence(file_path, n_mfcc=40, max_len=200):\n",
    "    y, sr = librosa.load(file_path, res_type='scipy')\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    # Pad or truncate to fixed length for batching\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0,0),(0,pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    return mfcc.T  # Shape: (max_len, n_mfcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b16b19b3-f17c-4711-a055-d292766ccdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {\n",
    "    '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
    "    '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
    "}\n",
    "observed_emotions = list(emotions.values())\n",
    "\n",
    "def load_data_dl(test_size=0.2, max_len=200):\n",
    "    x, y = [], []\n",
    "    # Update these paths for your dataset\n",
    "    speech_files = glob.glob('/Users/dushyantyadav/Downloads/Audio_Speech_Actors_01-24/Actor*/**/*.wav', recursive=True)\n",
    "    song_files = glob.glob('/Users/dushyantyadav/Downloads/Audio_Song_Actors_01-24/Actor*/**/*.wav', recursive=True)\n",
    "    all_files = speech_files + song_files\n",
    "    for file in all_files:\n",
    "        file_name = os.path.basename(file)\n",
    "        emotion_code = file_name.split(\"-\")[2]\n",
    "        emotion = emotions.get(emotion_code)\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        try:\n",
    "            mfcc_seq = extract_mfcc_sequence(file, max_len=max_len)\n",
    "            x.append(mfcc_seq)\n",
    "            y.append(emotion)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)\n",
    "    y_cat = to_categorical(y_enc)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y_cat, test_size=test_size, random_state=42, stratify=y)\n",
    "    return x_train, x_test, y_train, y_test, le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6fcc5abc-a385-4c56-87c4-6fe67a7ca2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech files found: 1440\n",
      "Song files found: 1012\n",
      "Total files found: 2452\n"
     ]
    }
   ],
   "source": [
    "speech_files = glob.glob('/Users/dushyantyadav/Downloads/Audio_Speech_Actors_01-24/Actor*/**/*.wav', recursive=True)\n",
    "song_files = glob.glob('/Users/dushyantyadav/Downloads/Audio_Song_Actors_01-24/Actor*/**/*.wav', recursive=True)\n",
    "all_files = speech_files + song_files\n",
    "print(\"Speech files found:\", len(speech_files))\n",
    "print(\"Song files found:\", len(song_files))\n",
    "print(\"Total files found:\", len(all_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3c40f966-1dac-417f-9f66-25983c7cd2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_lstm(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, kernel_size=5, activation='relu', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(128, kernel_size=5, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(128, return_sequences=False))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d5d561fb-9571-4431-96c2-c12e7aa88c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "62/62 [==============================] - 8s 51ms/step - loss: 1.9759 - accuracy: 0.2300 - val_loss: 2.4841 - val_accuracy: 0.1527\n",
      "Epoch 2/60\n",
      "62/62 [==============================] - 1s 24ms/step - loss: 1.7824 - accuracy: 0.2917 - val_loss: 2.3409 - val_accuracy: 0.1690\n",
      "Epoch 3/60\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 1.6413 - accuracy: 0.3345 - val_loss: 1.6448 - val_accuracy: 0.3381\n",
      "Epoch 4/60\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 1.5676 - accuracy: 0.3886 - val_loss: 1.4623 - val_accuracy: 0.4379\n",
      "Epoch 5/60\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 1.4874 - accuracy: 0.4146 - val_loss: 1.5772 - val_accuracy: 0.3747\n",
      "Epoch 6/60\n",
      "62/62 [==============================] - 1s 24ms/step - loss: 1.4309 - accuracy: 0.4442 - val_loss: 1.3819 - val_accuracy: 0.4542\n",
      "Epoch 7/60\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 1.3763 - accuracy: 0.4640 - val_loss: 1.4152 - val_accuracy: 0.4460\n",
      "Epoch 8/60\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 1.2865 - accuracy: 0.5054 - val_loss: 1.4366 - val_accuracy: 0.4134\n",
      "Epoch 9/60\n",
      "62/62 [==============================] - 1s 24ms/step - loss: 1.2654 - accuracy: 0.5242 - val_loss: 1.5495 - val_accuracy: 0.4155\n",
      "Epoch 10/60\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 1.2053 - accuracy: 0.5482 - val_loss: 1.2897 - val_accuracy: 0.5234\n",
      "Epoch 11/60\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 1.0856 - accuracy: 0.5997 - val_loss: 1.2882 - val_accuracy: 0.5642\n",
      "Epoch 12/60\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 1.0019 - accuracy: 0.6242 - val_loss: 1.3173 - val_accuracy: 0.4949\n",
      "Epoch 13/60\n",
      "62/62 [==============================] - 1s 24ms/step - loss: 0.9599 - accuracy: 0.6578 - val_loss: 1.1353 - val_accuracy: 0.5906\n",
      "Epoch 14/60\n",
      "62/62 [==============================] - 2s 24ms/step - loss: 0.9237 - accuracy: 0.6594 - val_loss: 1.3864 - val_accuracy: 0.5356\n",
      "Epoch 15/60\n",
      "62/62 [==============================] - 1s 24ms/step - loss: 0.8385 - accuracy: 0.6966 - val_loss: 0.9544 - val_accuracy: 0.6293\n",
      "Epoch 16/60\n",
      "62/62 [==============================] - 1s 24ms/step - loss: 0.7283 - accuracy: 0.7364 - val_loss: 1.2665 - val_accuracy: 0.5519\n",
      "Epoch 17/60\n",
      "62/62 [==============================] - 1s 24ms/step - loss: 0.7308 - accuracy: 0.7323 - val_loss: 0.8442 - val_accuracy: 0.6843\n",
      "Epoch 18/60\n",
      "62/62 [==============================] - 1s 24ms/step - loss: 0.7139 - accuracy: 0.7348 - val_loss: 0.8207 - val_accuracy: 0.6945\n",
      "Epoch 19/60\n",
      "62/62 [==============================] - 1s 24ms/step - loss: 0.6780 - accuracy: 0.7522 - val_loss: 0.8264 - val_accuracy: 0.7189\n",
      "Epoch 20/60\n",
      "62/62 [==============================] - 1s 24ms/step - loss: 0.6328 - accuracy: 0.7639 - val_loss: 0.9499 - val_accuracy: 0.6762\n",
      "Epoch 21/60\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 0.6275 - accuracy: 0.7700 - val_loss: 0.8942 - val_accuracy: 0.6884\n",
      "Epoch 22/60\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.5963 - accuracy: 0.7909 - val_loss: 0.9502 - val_accuracy: 0.6864\n",
      "Epoch 23/60\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 0.5452 - accuracy: 0.8093 - val_loss: 0.9757 - val_accuracy: 0.6904\n",
      "Epoch 24/60\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 0.5922 - accuracy: 0.7812 - val_loss: 0.8427 - val_accuracy: 0.7088\n",
      "Epoch 25/60\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 0.5114 - accuracy: 0.8174 - val_loss: 0.7736 - val_accuracy: 0.7373\n",
      "Epoch 26/60\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 0.5144 - accuracy: 0.8190 - val_loss: 1.0973 - val_accuracy: 0.6741\n",
      "Epoch 27/60\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 0.4977 - accuracy: 0.8241 - val_loss: 0.8168 - val_accuracy: 0.7312\n",
      "Epoch 28/60\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.4529 - accuracy: 0.8343 - val_loss: 0.8455 - val_accuracy: 0.7230\n",
      "Epoch 29/60\n",
      "62/62 [==============================] - 2s 24ms/step - loss: 0.4524 - accuracy: 0.8353 - val_loss: 0.7614 - val_accuracy: 0.7332\n",
      "Epoch 30/60\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 0.4361 - accuracy: 0.8455 - val_loss: 0.8503 - val_accuracy: 0.7210\n",
      "Epoch 31/60\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 0.3773 - accuracy: 0.8679 - val_loss: 0.9531 - val_accuracy: 0.7108\n",
      "Epoch 32/60\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 0.4007 - accuracy: 0.8567 - val_loss: 0.6922 - val_accuracy: 0.7800\n",
      "Epoch 33/60\n",
      "62/62 [==============================] - 1s 24ms/step - loss: 0.3759 - accuracy: 0.8654 - val_loss: 0.8954 - val_accuracy: 0.7149\n",
      "Epoch 34/60\n",
      "62/62 [==============================] - 1s 24ms/step - loss: 0.3278 - accuracy: 0.8893 - val_loss: 0.7950 - val_accuracy: 0.7556\n",
      "Epoch 35/60\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.3524 - accuracy: 0.8781 - val_loss: 0.7276 - val_accuracy: 0.7495\n",
      "Epoch 36/60\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.3064 - accuracy: 0.8914 - val_loss: 0.8295 - val_accuracy: 0.7434\n",
      "Epoch 37/60\n",
      "62/62 [==============================] - 2s 24ms/step - loss: 0.3520 - accuracy: 0.8797 - val_loss: 0.8450 - val_accuracy: 0.7413\n",
      "Epoch 38/60\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.3196 - accuracy: 0.8965 - val_loss: 0.9522 - val_accuracy: 0.7189\n",
      "Epoch 39/60\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 0.2717 - accuracy: 0.9148 - val_loss: 1.0401 - val_accuracy: 0.7210\n",
      "Epoch 40/60\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.2805 - accuracy: 0.9174 - val_loss: 0.9166 - val_accuracy: 0.7312\n",
      "Epoch 41/60\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.2549 - accuracy: 0.9184 - val_loss: 0.7470 - val_accuracy: 0.7780\n",
      "Epoch 42/60\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.2881 - accuracy: 0.9052 - val_loss: 0.7898 - val_accuracy: 0.7699\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.6922 - accuracy: 0.7800\n",
      "Test accuracy: 0.7800407409667969\n",
      "16/16 [==============================] - 1s 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.93      0.83      0.87        75\n",
      "        calm       0.90      0.85      0.88        75\n",
      "     disgust       0.79      0.69      0.74        39\n",
      "     fearful       0.76      0.73      0.75        75\n",
      "       happy       0.79      0.84      0.81        75\n",
      "     neutral       0.76      0.76      0.76        38\n",
      "         sad       0.61      0.68      0.65        75\n",
      "   surprised       0.70      0.82      0.75        39\n",
      "\n",
      "    accuracy                           0.78       491\n",
      "   macro avg       0.78      0.78      0.78       491\n",
      "weighted avg       0.79      0.78      0.78       491\n",
      "\n",
      "Confusion Matrix:\n",
      " [[62  0  6  3  1  0  0  3]\n",
      " [ 0 64  0  0  2  3  6  0]\n",
      " [ 2  0 27  1  0  1  6  2]\n",
      " [ 1  0  0 55  5  0 12  2]\n",
      " [ 0  2  1  1 63  3  0  5]\n",
      " [ 0  0  0  0  0 29  8  1]\n",
      " [ 1  5  0 11  4  2 51  1]\n",
      " [ 1  0  0  1  5  0  0 32]]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_train, x_test, y_train, y_test, le = load_data_dl(test_size=0.2, max_len=200)\n",
    "input_shape = x_train.shape[1:]  # (max_len, n_mfcc)\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "# Build model\n",
    "model = build_cnn_lstm(input_shape, num_classes)\n",
    "\n",
    "# Early stopping for better generalization\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=60,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", acc)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a8a4d84b-07b3-45cb-9241-ced20ea53d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_file = '/Users/dushyantyadav/Downloads/Crema/1001_IEO_DIS_LO.wav'\n",
    "mfcc_seq = extract_mfcc_sequence(test_file, max_len=200)  # Use your pipeline's function\n",
    "mfcc_seq = np.expand_dims(mfcc_seq, axis=0)  # Reshape for batch (1, max_len, n_mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9207d747-268b-427a-aff0-382a2a1467fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Predicted emotion: surprised\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(mfcc_seq)  # For Keras/TensorFlow models\n",
    "predicted_class = np.argmax(pred)\n",
    "predicted_emotion = le.classes_[predicted_class]\n",
    "print(\"Predicted emotion:\", predicted_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "46ff5434-81bd-46ba-8475-5c233d23afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_file = '/Users/dushyantyadav/Downloads/Crema/1083_IEO_ANG_MD.wav'\n",
    "mfcc_seq = extract_mfcc_sequence(test_file, max_len=200)  # Use your pipeline's function\n",
    "mfcc_seq = np.expand_dims(mfcc_seq, axis=0)  # Reshape for batch (1, max_len, n_mfcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "31ed0cc3-58b6-400a-9279-0545a65e4e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n",
      "Predicted emotion: surprised\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(mfcc_seq)  # For Keras/TensorFlow models\n",
    "predicted_class = np.argmax(pred)\n",
    "predicted_emotion = le.classes_[predicted_class]\n",
    "print(\"Predicted emotion:\", predicted_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f54ea3-4e2c-4966-9cfa-47296cd51860",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
