{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85436493-8385-4d35-acd8-91601c13e60a",
   "metadata": {},
   "source": [
    "# Speech Emotion Recognition Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6ace62-8fa0-49c2-a1d9-a721592900eb",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook implements a robust, end-to-end pipeline for emotion classification on speech data using deep learning. The objective is to accurately identify and categorize emotional states—such as happy, sad, angry, fearful, neutral, calm, disgust, and surprised—from both speech and song audio recordings. The workflow covers data loading, feature extraction, augmentation, model training, evaluation, and inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5396c03d-868d-4372-b798-a096a7f08a8a",
   "metadata": {},
   "source": [
    "## 2. Library Installation and Imports\n",
    "\n",
    "We install and import all required libraries for audio processing, data handling, feature extraction, machine learning, and deep learning. This includes `librosa`, `numpy`, `scikit-learn`, and `tensorflow.keras`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a68b63b-fe68-4390-a75a-5ff6a3f4791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506561a6-dc9c-445e-bfbf-915f76858a67",
   "metadata": {},
   "source": [
    "## 3. Emotion Mapping and Data Path Configuration\n",
    "\n",
    "We define the mapping between emotion codes and their corresponding labels, and specify the file paths for the speech and song audio datasets. This mapping is crucial for extracting the correct emotion label from each audio file's name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e02146b-b73b-47fc-8cef-158cdecc8160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion mapping (adapt for your dataset)\n",
    "emotions = {\n",
    "    '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
    "    '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
    "}\n",
    "observed_emotions = list(emotions.values())\n",
    "DATA_PATHS = [\n",
    "    '/Users/dushyantyadav/Downloads/Audio_Speech_Actors_01-24/Actor*/**/*.wav',\n",
    "    '/Users/dushyantyadav/Downloads/Audio_Song_Actors_01-24/Actor*/**/*.wav'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52275a3d-a6b0-4d05-ad59-db2e1aa2e2e4",
   "metadata": {},
   "source": [
    "## 4. Data Augmentation Functions\n",
    "\n",
    "To improve model robustness and address class imbalance, we define functions for audio augmentation, including adding noise and shifting the audio signal. Augmentation increases the diversity of training samples and helps the model generalize better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d64a366-f515-4a8c-87e0-170ff080b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(data, noise_factor=0.005):\n",
    "    noise = np.random.randn(len(data))\n",
    "    return data + noise_factor * noise\n",
    "\n",
    "def shift(data, shift_max=0.2, shift_direction='both'):\n",
    "    shift_amt = np.random.randint(int(len(data) * shift_max))\n",
    "    if shift_direction == 'right':\n",
    "        shift_amt = -shift_amt\n",
    "    elif shift_direction == 'both':\n",
    "        if np.random.randint(0, 2) == 1:\n",
    "            shift_amt = -shift_amt\n",
    "    return np.roll(data, shift_amt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e793a12f-24b7-4d2d-a12e-0bc54254ac88",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction\n",
    "\n",
    "We extract Mel Frequency Cepstral Coefficients (MFCCs) from each audio file, which serve as input features for the model. Each MFCC sequence is padded or truncated to a fixed length to ensure uniform input dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6198cdc7-1d0e-4650-bf76-c207a4a19c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_sequence(file_path, n_mfcc=40, max_len=200):\n",
    "    y, sr = librosa.load(file_path, res_type='scipy')\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    # Pad or truncate for batching\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0,0),(0,pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    return mfcc.T  # (max_len, n_mfcc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792a6070-3477-4ca8-b49f-82b795174230",
   "metadata": {},
   "source": [
    "## 6. Data Loading and Preprocessing\n",
    "\n",
    "We load all relevant audio files, extract their features, apply augmentation, and encode the emotion labels. The data is then split into training and testing sets using stratified sampling to preserve class distributions. Label encoding and one-hot encoding are used to prepare the targets for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41330cee-b980-42a2-a18f-086f7f2d9c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_dl(test_size=0.2, max_len=200, augment=True):\n",
    "    x, y = [], []\n",
    "    files = []\n",
    "    for path in DATA_PATHS:\n",
    "        files.extend(glob.glob(path, recursive=True))\n",
    "    for file in files:\n",
    "        file_name = os.path.basename(file)\n",
    "        emotion_code = file_name.split(\"-\")[2]\n",
    "        emotion = emotions.get(emotion_code)\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        try:\n",
    "            # Original\n",
    "            mfcc_seq = extract_mfcc_sequence(file, max_len=max_len)\n",
    "            x.append(mfcc_seq)\n",
    "            y.append(emotion)\n",
    "            if augment:\n",
    "                # Augmented: noise\n",
    "                y_audio, sr = librosa.load(file, res_type='scipy')\n",
    "                mfcc_noise = librosa.feature.mfcc(y=add_noise(y_audio), sr=sr, n_mfcc=40)\n",
    "                if mfcc_noise.shape[1] < max_len:\n",
    "                    pad_width = max_len - mfcc_noise.shape[1]\n",
    "                    mfcc_noise = np.pad(mfcc_noise, pad_width=((0,0),(0,pad_width)), mode='constant')\n",
    "                else:\n",
    "                    mfcc_noise = mfcc_noise[:, :max_len]\n",
    "                x.append(mfcc_noise.T)\n",
    "                y.append(emotion)\n",
    "                # Augmented: shift\n",
    "                y_shift = shift(y_audio)\n",
    "                mfcc_shift = librosa.feature.mfcc(y=y_shift, sr=sr, n_mfcc=40)\n",
    "                if mfcc_shift.shape[1] < max_len:\n",
    "                    pad_width = max_len - mfcc_shift.shape[1]\n",
    "                    mfcc_shift = np.pad(mfcc_shift, pad_width=((0,0),(0,pad_width)), mode='constant')\n",
    "                else:\n",
    "                    mfcc_shift = mfcc_shift[:, :max_len]\n",
    "                x.append(mfcc_shift.T)\n",
    "                y.append(emotion)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)\n",
    "    y_cat = to_categorical(y_enc)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y_cat, test_size=test_size, random_state=42, stratify=y)\n",
    "    return x_train, x_test, y_train, y_test, le, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d11718-2e5a-4421-b969-b69fe732fafe",
   "metadata": {},
   "source": [
    "## 7. Model Architecture: CNN-LSTM/GRU\n",
    "\n",
    "We define the deep learning model architecture, which combines 1D convolutional layers for feature extraction with either GRU or LSTM layers for capturing temporal dependencies in the audio data. Batch normalization and dropout are used for regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b977fa2-6dfa-4652-82a6-7912d40e4e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_lstm(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, kernel_size=5, activation='relu', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(128, kernel_size=5, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(128, return_sequences=False))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd5b7e-8495-4a91-9dfb-89273005710a",
   "metadata": {},
   "source": [
    "## 8. Class Weights Calculation\n",
    "\n",
    "To address class imbalance, we compute class weights based on the frequency of each class in the training data. These weights are used during model training to ensure that minority classes are not neglected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59da7cca-090f-40d5-99b2-d74420672c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.815410199556541, 1: 0.8145071982281284, 2: 1.5954446854663775, 3: 0.815410199556541, 4: 0.815410199556541, 5: 1.630820399113082, 6: 0.815410199556541, 7: 1.5954446854663775}\n"
     ]
    }
   ],
   "source": [
    "# After loading data\n",
    "x_train, x_test, y_train, y_test, le, y_all = load_data_dl(test_size=0.2, max_len=200, augment=True)\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_labels), y=y_train_labels)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(\"Class weights:\", class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323b4c0b-c2fb-41b1-919e-3d6901615bdc",
   "metadata": {},
   "source": [
    "## 9. Model Training\n",
    "\n",
    "The model is trained on the processed and augmented data. Early stopping is used to prevent overfitting by monitoring the validation loss, and class weights are applied to improve performance on minority classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a586e8e-1ac9-49ee-abeb-304483c115f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 21:49:57.622698: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-06-21 21:49:57.622960: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-06-21 21:49:57.622987: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-06-21 21:49:57.623362: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-06-21 21:49:57.623824: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 21:49:59.570151: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 8s 31ms/step - loss: 2.0172 - accuracy: 0.2039 - val_loss: 1.8619 - val_accuracy: 0.2772\n",
      "Epoch 2/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 1.7506 - accuracy: 0.3110 - val_loss: 1.6217 - val_accuracy: 0.3913\n",
      "Epoch 3/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 1.4754 - accuracy: 0.4369 - val_loss: 1.4525 - val_accuracy: 0.4436\n",
      "Epoch 4/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 1.3165 - accuracy: 0.4944 - val_loss: 1.2939 - val_accuracy: 0.5082\n",
      "Epoch 5/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 1.2301 - accuracy: 0.5318 - val_loss: 1.2052 - val_accuracy: 0.5306\n",
      "Epoch 6/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 1.1058 - accuracy: 0.5766 - val_loss: 1.0416 - val_accuracy: 0.6080\n",
      "Epoch 7/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.9971 - accuracy: 0.6101 - val_loss: 1.0168 - val_accuracy: 0.6230\n",
      "Epoch 8/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.9378 - accuracy: 0.6329 - val_loss: 0.9854 - val_accuracy: 0.6155\n",
      "Epoch 9/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.8837 - accuracy: 0.6555 - val_loss: 0.8855 - val_accuracy: 0.6726\n",
      "Epoch 10/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.8551 - accuracy: 0.6611 - val_loss: 1.0103 - val_accuracy: 0.6372\n",
      "Epoch 11/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.8230 - accuracy: 0.6793 - val_loss: 0.8333 - val_accuracy: 0.6929\n",
      "Epoch 12/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.7574 - accuracy: 0.7029 - val_loss: 0.9016 - val_accuracy: 0.6658\n",
      "Epoch 13/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.7348 - accuracy: 0.7175 - val_loss: 0.8794 - val_accuracy: 0.6841\n",
      "Epoch 14/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.7163 - accuracy: 0.7248 - val_loss: 0.8686 - val_accuracy: 0.6929\n",
      "Epoch 15/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.6837 - accuracy: 0.7354 - val_loss: 0.7429 - val_accuracy: 0.7330\n",
      "Epoch 16/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.6576 - accuracy: 0.7459 - val_loss: 0.7098 - val_accuracy: 0.7310\n",
      "Epoch 17/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.6461 - accuracy: 0.7498 - val_loss: 0.7318 - val_accuracy: 0.7289\n",
      "Epoch 18/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.6346 - accuracy: 0.7536 - val_loss: 0.6885 - val_accuracy: 0.7378\n",
      "Epoch 19/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.5647 - accuracy: 0.7838 - val_loss: 0.7534 - val_accuracy: 0.7303\n",
      "Epoch 20/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.5612 - accuracy: 0.7845 - val_loss: 0.6030 - val_accuracy: 0.7779\n",
      "Epoch 21/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.5351 - accuracy: 0.7954 - val_loss: 0.6487 - val_accuracy: 0.7704\n",
      "Epoch 22/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.5351 - accuracy: 0.7984 - val_loss: 0.5648 - val_accuracy: 0.8071\n",
      "Epoch 23/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.5048 - accuracy: 0.8081 - val_loss: 0.5656 - val_accuracy: 0.7921\n",
      "Epoch 24/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.4982 - accuracy: 0.8166 - val_loss: 0.6258 - val_accuracy: 0.7711\n",
      "Epoch 25/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.4825 - accuracy: 0.8188 - val_loss: 0.5579 - val_accuracy: 0.8084\n",
      "Epoch 26/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.4684 - accuracy: 0.8294 - val_loss: 0.6193 - val_accuracy: 0.7779\n",
      "Epoch 27/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.4618 - accuracy: 0.8319 - val_loss: 0.6452 - val_accuracy: 0.7819\n",
      "Epoch 28/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.4070 - accuracy: 0.8482 - val_loss: 0.6116 - val_accuracy: 0.7833\n",
      "Epoch 29/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.4018 - accuracy: 0.8477 - val_loss: 0.5912 - val_accuracy: 0.7969\n",
      "Epoch 30/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.4045 - accuracy: 0.8448 - val_loss: 0.5700 - val_accuracy: 0.8091\n",
      "Epoch 31/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.3800 - accuracy: 0.8637 - val_loss: 0.4663 - val_accuracy: 0.8363\n",
      "Epoch 32/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.3670 - accuracy: 0.8615 - val_loss: 0.5131 - val_accuracy: 0.8159\n",
      "Epoch 33/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.3559 - accuracy: 0.8651 - val_loss: 0.4558 - val_accuracy: 0.8451\n",
      "Epoch 34/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.3639 - accuracy: 0.8693 - val_loss: 0.5204 - val_accuracy: 0.8234\n",
      "Epoch 35/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.3543 - accuracy: 0.8676 - val_loss: 0.6222 - val_accuracy: 0.8166\n",
      "Epoch 36/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.3186 - accuracy: 0.8843 - val_loss: 0.4591 - val_accuracy: 0.8451\n",
      "Epoch 37/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.3203 - accuracy: 0.8834 - val_loss: 0.4807 - val_accuracy: 0.8438\n",
      "Epoch 38/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.3138 - accuracy: 0.8809 - val_loss: 0.4287 - val_accuracy: 0.8478\n",
      "Epoch 39/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.3159 - accuracy: 0.8815 - val_loss: 0.5414 - val_accuracy: 0.8288\n",
      "Epoch 40/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.2920 - accuracy: 0.8926 - val_loss: 0.5384 - val_accuracy: 0.8302\n",
      "Epoch 41/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.2938 - accuracy: 0.8872 - val_loss: 0.4504 - val_accuracy: 0.8587\n",
      "Epoch 42/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.3149 - accuracy: 0.8865 - val_loss: 0.4070 - val_accuracy: 0.8621\n",
      "Epoch 43/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.2695 - accuracy: 0.9038 - val_loss: 0.4167 - val_accuracy: 0.8702\n",
      "Epoch 44/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.2408 - accuracy: 0.9138 - val_loss: 0.4571 - val_accuracy: 0.8553\n",
      "Epoch 45/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.2823 - accuracy: 0.9019 - val_loss: 0.4360 - val_accuracy: 0.8580\n",
      "Epoch 46/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.2567 - accuracy: 0.9060 - val_loss: 0.5442 - val_accuracy: 0.8356\n",
      "Epoch 47/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.2229 - accuracy: 0.9198 - val_loss: 0.4830 - val_accuracy: 0.8553\n",
      "Epoch 48/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.2443 - accuracy: 0.9121 - val_loss: 0.3745 - val_accuracy: 0.8770\n",
      "Epoch 49/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.2368 - accuracy: 0.9130 - val_loss: 0.3859 - val_accuracy: 0.8723\n",
      "Epoch 50/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.2293 - accuracy: 0.9208 - val_loss: 0.5106 - val_accuracy: 0.8478\n",
      "Epoch 51/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.2315 - accuracy: 0.9181 - val_loss: 0.4354 - val_accuracy: 0.8675\n",
      "Epoch 52/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.2259 - accuracy: 0.9198 - val_loss: 0.3363 - val_accuracy: 0.8981\n",
      "Epoch 53/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.2032 - accuracy: 0.9308 - val_loss: 0.3540 - val_accuracy: 0.8825\n",
      "Epoch 54/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.2002 - accuracy: 0.9244 - val_loss: 0.3664 - val_accuracy: 0.8913\n",
      "Epoch 55/60\n",
      "184/184 [==============================] - 5s 25ms/step - loss: 0.2054 - accuracy: 0.9256 - val_loss: 0.4094 - val_accuracy: 0.8777\n",
      "Epoch 56/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.1968 - accuracy: 0.9290 - val_loss: 0.4801 - val_accuracy: 0.8492\n",
      "Epoch 57/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.2129 - accuracy: 0.9203 - val_loss: 0.3881 - val_accuracy: 0.8716\n",
      "Epoch 58/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.2004 - accuracy: 0.9274 - val_loss: 0.3725 - val_accuracy: 0.8804\n",
      "Epoch 59/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.1662 - accuracy: 0.9410 - val_loss: 0.4075 - val_accuracy: 0.8777\n",
      "Epoch 60/60\n",
      "184/184 [==============================] - 5s 26ms/step - loss: 0.1917 - accuracy: 0.9335 - val_loss: 0.3324 - val_accuracy: 0.8906\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.3324 - accuracy: 0.8906\n",
      "Test accuracy: 0.890625\n",
      "46/46 [==============================] - 1s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.98      0.90      0.94       226\n",
      "        calm       0.89      0.94      0.92       225\n",
      "     disgust       0.94      0.85      0.89       115\n",
      "     fearful       0.90      0.84      0.86       226\n",
      "       happy       0.92      0.88      0.90       226\n",
      "     neutral       0.78      0.92      0.85       113\n",
      "         sad       0.83      0.85      0.84       226\n",
      "   surprised       0.87      0.97      0.92       115\n",
      "\n",
      "    accuracy                           0.89      1472\n",
      "   macro avg       0.89      0.90      0.89      1472\n",
      "weighted avg       0.89      0.89      0.89      1472\n",
      "\n",
      "Confusion Matrix:\n",
      " [[203   1   3   7   6   1   5   0]\n",
      " [  0 212   0   0   3   7   3   0]\n",
      " [  3   5  98   1   2   1   3   2]\n",
      " [  1   1   1 189   3   1  24   6]\n",
      " [  0   2   1   6 200   9   2   6]\n",
      " [  0   5   0   0   0 104   2   2]\n",
      " [  0  12   0   7   4   9 193   1]\n",
      " [  0   0   1   1   0   1   0 112]]\n"
     ]
    }
   ],
   "source": [
    "input_shape = x_train.shape[1:]  # (max_len, n_mfcc)\n",
    "num_classes = y_train.shape[1]\n",
    "model = build_cnn_lstm(input_shape, num_classes)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=60,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weight_dict\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", acc)\n",
    "\n",
    "y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7cec7d-e731-4f08-9bdf-990dc82b5b28",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation\n",
    "\n",
    "After training, we evaluate the model on the test set. We report the overall accuracy, per-class precision, recall, F1-score, and present the confusion matrix. These metrics are used to ensure the model meets the project criteria for balanced and accurate emotion classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ebf614-4ab6-4a01-8e68-7c2ad26b5f8d",
   "metadata": {},
   "source": [
    "## 11. Inference and Prediction\n",
    "\n",
    "We implement a function for predicting the emotion of a single audio file using the trained model. This function will be used for both standalone inference scripts and integration into the web application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9db3280e-2171-4bbe-845a-77b74f03830b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 62ms/step\n",
      "Predicted emotion: fearful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_emotion(file_path, model, le, max_len=200):\n",
    "    mfcc_seq = extract_mfcc_sequence(file_path, max_len=max_len)\n",
    "    mfcc_seq = np.expand_dims(mfcc_seq, axis=0)\n",
    "    pred = model.predict(mfcc_seq)\n",
    "    predicted_class = np.argmax(pred)\n",
    "    return le.classes_[predicted_class]\n",
    "\n",
    "# Example usage\n",
    "test_file = '/Users/dushyantyadav/Downloads/Crema/1082_IEO_FEA_MD.wav'\n",
    "print(\"Predicted emotion:\", predict_emotion(test_file, model, le))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a19735c-4219-47be-944b-08b96aa16baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training your model\n",
    "model.save(\"model.keras\")  # Recommended Keras v3 format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3d9299-e3f1-4190-a0df-97d7c3a1480c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
